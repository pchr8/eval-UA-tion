
@misc{cong_have_2024,
	title = {Have You Merged My Model? On The Robustness of Large Language Model {IP} Protection Methods Against Model Merging},
	url = {http://arxiv.org/abs/2404.05188},
	shorttitle = {Have You Merged My Model?},
	abstract = {Model merging is a promising lightweight model empowerment technique that does not rely on expensive computing devices (e.g., {GPUs}) or require the collection of specific training data. Instead, it involves editing different upstream model parameters to absorb their downstream task capabilities. However, uncertified model merging can infringe upon the Intellectual Property ({IP}) rights of the original upstream models. In this paper, we conduct the first study on the robustness of {IP} protection methods in model merging scenarios. We investigate two state-of-the-art {IP} protection techniques: Quantization Watermarking and Instructional Fingerprint, along with various advanced model merging technologies, such as Task Arithmetic, {TIES}-{MERGING}, and so on. Experimental results indicate that current Large Language Model ({LLM}) watermarking techniques cannot survive in the merged models, whereas model fingerprinting techniques can. Our research aims to highlight that model merging should be an indispensable consideration in the robustness assessment of model {IP} protection techniques, thereby promoting the healthy development of the open-source {LLM} community.},
	number = {{arXiv}:2404.05188},
	publisher = {{arXiv}},
	author = {Cong, Tianshuo and Ran, Delong and Liu, Zesen and He, Xinlei and Liu, Jinyuan and Gong, Yichen and Li, Qi and Wang, Anyu and Wang, Xiaoyun},
	urldate = {2024-04-18},
	date = {2024-04-08},
	eprinttype = {arxiv},
	eprint = {2404.05188 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{araci_finbert_2019,
	title = {{FinBERT}: Financial Sentiment Analysis with Pre-trained Language Models},
	url = {http://arxiv.org/abs/1908.10063},
	shorttitle = {{FinBERT}},
	abstract = {Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce {FinBERT}, a language model based on {BERT}, to tackle {NLP} tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, {FinBERT} outperforms state-of-the-art machine learning methods.},
	number = {{arXiv}:1908.10063},
	publisher = {{arXiv}},
	author = {Araci, Dogu},
	urldate = {2023-09-11},
	date = {2019-08-27},
	eprinttype = {arxiv},
	eprint = {1908.10063 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, lm, nlp},
}

@misc{koubaa_gpt-4_2023,
	title = {{GPT}-4 vs. {GPT}-3.5: A Concise Showdown},
	rights = {http://creativecommons.org/licenses/by/4.0},
	url = {https://www.preprints.org/manuscript/202303.0422/v1},
	doi = {10.20944/preprints202303.0422.v1},
	shorttitle = {{GPT}-4 vs. {GPT}-3.5},
	abstract = {As the echoes of {ChatGPT}'s remarkable success continue to permeate the {AI} community, its formidable successor, {GPT}-4, has emerged, showing off a wealth of novel features. In this concise paper, we elucidate the capabilities of {GPT}-4 and conduct a comparative analysis with its predecessor, {ChatGPT}, offering insights into their relative strengths and advancements in the rapidly evolving field of generative {AI}. We also present a comprehensive summary of the major performance results reported by {OpenAI} on {GPT}-4 across various Natural Language Processing ({NLP}) tasks. We place great emphasis on the innovative advancements offered by {GPT}-4 in comparison to its predecessors. Our focus is on highlighting its remarkable performance while also mentioning its limitations. The purpose of this paper is to deliver a succinct understanding of the new features and performance benchmarks of {GPT}-4.},
	author = {Koubaa, Anis},
	urldate = {2024-04-16},
	date = {2023-03-24},
}

@misc{OpenOrca,
	title = {{OpenOrca}: An open dataset of {GPT} augmented {FLAN} reasoning traces},
	url = {https://https://huggingface.co/Open-Orca/OpenOrca},
	publisher = {{HuggingFace}},
	author = {Lian, Wing and Goodson, Bleys and Pentland, Eugene and Cook, Austin and Vong, Chanvichet and {"Teknium"}},
	date = {2023},
}

@inproceedings{sherlock,
	location = {Torino, Italy},
	title = {Fine-tuning and retrieval augmented generation for question answering using affordable large language models},
	booktitle = {to appear in Proceedings of the third ukrainian natural language processing workshop},
	publisher = {European Language Resources Association},
	author = {Boros, Tiberiu and Chivereanu, Radu and Dumitrescu, Stefan and Purcaru, Octavian},
	date = {2024-05},
}

@misc{Amelina_2022,
	title = {Cancel culture vs. execute culture},
	url = {https://www.eurozine.com/cancel-culture-vs-execute-culture/},
	author = {Amelina, Viktoria},
	date = {2022-03},
}

@article{hakkani-tur_statistical_2002,
	title = {Statistical Morphological Disambiguation for Agglutinative Languages},
	volume = {36},
	issn = {1572-8412},
	url = {https://doi.org/10.1023/A:1020271707826},
	doi = {10.1023/A:1020271707826},
	abstract = {We present statistical models for morphological disambiguation in agglutinative languages, with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested, the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95\% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features, then the accuracy increases to 95.07\%.},
	pages = {381--410},
	number = {4},
	journaltitle = {Computers and the Humanities},
	shortjournal = {Computers and the Humanities},
	author = {Hakkani-Tür, Dilek Z. and Oflazer, Kemal and Tür, Gökhan},
	date = {2002-11-01},
}

@article{hakkani-tur_no_2002,
	title = {[No title found]},
	volume = {36},
	issn = {00104817},
	url = {http://link.springer.com/10.1023/A:1020271707826},
	doi = {10.1023/A:1020271707826},
	pages = {381--410},
	number = {4},
	journaltitle = {Computers and the Humanities},
	author = {Hakkani-Tür, Dilek Z. and Oflazer, Kemal and Tür, Gökhan},
	urldate = {2024-04-15},
	date = {2002},
}

@article{hakkani-tur_no_2002-1,
	title = {[No title found]},
	volume = {36},
	issn = {00104817},
	url = {http://link.springer.com/10.1023/A:1020271707826},
	doi = {10.1023/A:1020271707826},
	pages = {381--410},
	number = {4},
	journaltitle = {Computers and the Humanities},
	author = {Hakkani-Tür, Dilek Z. and Oflazer, Kemal and Tür, Gökhan},
	urldate = {2024-04-15},
	date = {2002},
}

@article{ilson_introduction_1988,
	title = {Introduction},
	volume = {1},
	issn = {0950-3846, 1477-4577},
	url = {https://academic.oup.com/ijl/article-lookup/doi/10.1093/ijl/1.1.1-s},
	doi = {10.1093/ijl/1.1.1-s},
	pages = {1--s--1},
	number = {1},
	journaltitle = {International Journal of Lexicography},
	shortjournal = {Int J Lexicography},
	author = {Ilson, Robert},
	urldate = {2024-04-15},
	date = {1988},
	langid = {english},
}

@article{schick_true_2022,
	title = {True Few-Shot Learning with Prompts—A Real-World Perspective},
	volume = {10},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00485/111728/True-Few-Shot-Learning-with-Prompts-A-Real-World},
	doi = {10.1162/tacl_a_00485},
	abstract = {Abstract
            Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet’s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on {RAFT}, a benchmark of tasks taken from realistic {NLP} applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on {RAFT} and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.},
	pages = {716--731},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Schick, Timo and Schütze, Hinrich},
	urldate = {2024-04-15},
	date = {2022-06-17},
	langid = {english},
}

@inproceedings{zhang_improving_2020,
	location = {Online},
	title = {Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation},
	url = {https://www.aclweb.org/anthology/2020.acl-main.148},
	doi = {10.18653/v1/2020.acl-main.148},
	eventtitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	pages = {1628--1639},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Biao and Williams, Philip and Titov, Ivan and Sennrich, Rico},
	urldate = {2024-04-14},
	date = {2020},
	langid = {english},
}

@article{shvedova2017grac,
	title = {{GRAC}: General regionally annotated corpus of ukrainian},
	volume = {2022},
	journaltitle = {Electronic resource: Kyiv, Lviv, Jena},
	author = {Shvedova, M and von Waldenfels, R and Yarygin, S and Rysin, A and Starko, V and Nikolajenko, T and {others}},
	date = {2017},
}

@misc{schott_polyglot_2023,
	title = {Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models},
	url = {http://arxiv.org/abs/2305.13675},
	shorttitle = {Polyglot or Not?},
	abstract = {In this work, we assess the ability of foundation models to recall encyclopedic knowledge across a wide range of linguistic contexts. To support this, we: 1) produce a 20-language dataset that contains 303k factual associations paired with counterfactuals, 2) evaluate 5 models in a multilingual test, and 3) benchmark a diverse set of 24 models in an English-only test. Meta's {LLaMA} achieves the highest scores in both multilingual and English-only evaluations. Yet, an analysis of {LLaMA}'s errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects. Overall, our findings suggest that today's foundation models are far from polyglots.},
	number = {{arXiv}:2305.13675},
	publisher = {{arXiv}},
	author = {Schott, Tim and Furman, Daniel and Bhat, Shreshta},
	urldate = {2024-04-14},
	date = {2023-12-05},
	eprinttype = {arxiv},
	eprint = {2305.13675 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bandarkar_belebele_2023,
	title = {The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},
	url = {http://arxiv.org/abs/2308.16884},
	shorttitle = {The Belebele Benchmark},
	abstract = {We present Belebele, a multiple-choice machine reading comprehension ({MRC}) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding ({NLU}) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models ({MLMs}) and large language models ({LLMs}). We present extensive results and find that despite significant cross-lingual transfer in English-centric {LLMs}, much smaller {MLMs} pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of {NLP} systems.},
	number = {{arXiv}:2308.16884},
	publisher = {{arXiv}},
	author = {Bandarkar, Lucas and Liang, Davis and Muller, Benjamin and Artetxe, Mikel and Shukla, Satya Narayan and Husa, Donald and Goyal, Naman and Krishnan, Abhinandan and Zettlemoyer, Luke and Khabsa, Madian},
	urldate = {2024-04-14},
	date = {2023-08-31},
	eprinttype = {arxiv},
	eprint = {2308.16884 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
}

@article{nllb-22,
	title = {No language left behind: Scaling human-centered machine translation},
	author = {{NLLB Team} and Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Mejia-Gonzalez, Gabriel and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
	date = {2022},
	note = {tex.eprint: {arXiv}:1902.01382},
}

@inproceedings{djinni,
	location = {Torino, Italy},
	title = {Introducing the Djinni Recruitment Dataset: A corpus of anonymized {CVs} and job postings},
	booktitle = {Proceedings of the third ukrainian natural language processing workshop},
	publisher = {European Language Resources Association},
	author = {Drushchak, Nazarii and Romanyshyn, Mariana},
	date = {2024-05},
}

@inproceedings{conneau-etal-2020-unsupervised,
	location = {Online},
	title = {Unsupervised cross-lingual representation learning at scale},
	url = {https://aclanthology.org/2020.acl-main.747},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered {CommonCrawl} data. Our model, dubbed {XLM}-R, significantly outperforms multilingual {BERT} ({mBERT}) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on {XNLI}, +13\% average F1 score on {MLQA}, and +2.4\% F1 score on {NER}. {XLM}-R performs particularly well on low-resource languages, improving 15.7\% in {XNLI} accuracy for Swahili and 11.4\% for Urdu over previous {XLM} models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; {XLM}-R is very competitive with strong monolingual models on the {GLUE} and {XNLI} benchmarks. We will make our code and models publicly available.},
	pages = {8440--8451},
	booktitle = {Proceedings of the 58th annual meeting of the association for computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	date = {2020-07},
}

@misc{goddard_arcees_2024,
	title = {Arcee's {MergeKit}: A Toolkit for Merging Large Language Models},
	url = {http://arxiv.org/abs/2403.13257},
	shorttitle = {Arcee's {MergeKit}},
	abstract = {The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pretrained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in {AI} - including the difficulties of catastrophic forgetting and multitask learning. To support this expanding area of research, we introduce {MergeKit}, a comprehensive, open-source library designed to facilitate the application of model merging strategies. {MergeKit} offers an extensible framework to efficiently merge models on any hardware, providing utility to researchers and practitioners. To date, thousands of models have been merged by the open-source community, leading to the creation of some of the worlds most powerful open-source model checkpoints, as assessed by the Open {LLM} Leaderboard. The library is accessible at https://github.com/arcee-ai/{MergeKit}.},
	number = {{arXiv}:2403.13257},
	publisher = {{arXiv}},
	author = {Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and {McQuade}, Mark and Solawetz, Jacob},
	urldate = {2024-04-14},
	date = {2024-03-20},
	eprinttype = {arxiv},
	eprint = {2403.13257 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jiang_mistral_2023,
	title = {Mistral 7B},
	url = {http://arxiv.org/abs/2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention ({GQA}) for faster inference, coupled with sliding window attention ({SWA}) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	number = {{arXiv}:2310.06825},
	publisher = {{arXiv}},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	urldate = {2024-04-14},
	date = {2023-10-10},
	eprinttype = {arxiv},
	eprint = {2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{inproceedings,
	title = {How enterprises play: Towards a taxonomy for enterprise gamification},
	author = {Raftopoulos, Marigo},
	date = {2015-05},
}

@book{manning1999foundations,
	title = {Foundations of statistical natural language processing},
	isbn = {978-0-262-13360-9},
	url = {https://books.google.de/books?id=YiFDxbEX3SUC},
	series = {Foundations of statistical natural language processing},
	publisher = {{MIT} Press},
	author = {Manning, C. and Schutze, H.},
	date = {1999},
	note = {tex.lccn: 99021137},
}

@inproceedings{bhansali_language_2022,
	location = {Greater Noida, India},
	title = {Language Identification Using Combination of Machine Learning Algorithms and Vectorization Techniques},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66543-789-9},
	url = {https://ieeexplore.ieee.org/document/9823628/},
	doi = {10.1109/ICACITE53722.2022.9823628},
	eventtitle = {2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering ({ICACITE})},
	pages = {1329--1334},
	booktitle = {2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering ({ICACITE})},
	publisher = {{IEEE}},
	author = {Bhansali, Anushri and Chandravadiya, Amit and Panchal, Brijeshkumar Y. and Bohara, Mohammed Husain and Ganatra, Amit},
	urldate = {2024-04-13},
	date = {2022-04-28},
}

@article{scikit-learn,
	title = {Scikit-learn: Machine learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
}

@article{serasset_dbnary_2015,
	title = {{DBnary}: Wiktionary as a Lemon-based multilingual lexical resource in {RDF}},
	volume = {6},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-140147},
	doi = {10.3233/SW-140147},
	shorttitle = {{DBnary}},
	pages = {355--361},
	number = {4},
	journaltitle = {Semantic Web},
	shortjournal = {{SW}},
	author = {Sérasset, Gilles},
	editor = {Hellmann, Sebastian and Moran, Steven and Brümmer, Martin and {McCrae}, John},
	urldate = {2024-03-02},
	date = {2015-08-07},
}

@book{ramoo2021psychology,
	title = {Psychology of language},
	publisher = {{BCcampus}, {BC} Open Textbook Project},
	author = {Ramoo, Dinesh},
	date = {2021},
}

@book{drout2012drout,
	title = {Drout's quick and easy old english},
	url = {https://books.google.de/books?id=mJgDDQAAQBAJ},
	publisher = {Witan Publishing},
	author = {Drout, M.D.C. and Gilchrist, B.D. and Kapelle, R.},
	date = {2012},
}

@inproceedings{babych_unsupervised_2019,
	location = {Florence, Italy},
	title = {Unsupervised Induction of Ukrainian Morphological Paradigms for the New Lexicon: Extending Coverage for Named Entities and Neologisms using Inflection Tables and Unannotated Corpora},
	url = {https://www.aclweb.org/anthology/W19-3701},
	doi = {10.18653/v1/W19-3701},
	shorttitle = {Unsupervised Induction of Ukrainian Morphological Paradigms for the New Lexicon},
	eventtitle = {Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing},
	pages = {1--11},
	booktitle = {Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Babych, Bogdan},
	urldate = {2024-04-11},
	date = {2019},
	langid = {english},
}

@misc{albalak_survey_2024,
	title = {A Survey on Data Selection for Language Models},
	url = {http://arxiv.org/abs/2402.16827},
	abstract = {A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.},
	number = {{arXiv}:2402.16827},
	publisher = {{arXiv}},
	author = {Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and Raffel, Colin and Chang, Shiyu and Hashimoto, Tatsunori and Wang, William Yang},
	urldate = {2024-04-11},
	date = {2024-03-08},
	eprinttype = {arxiv},
	eprint = {2402.16827 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, todo, useful},
}

@software{lintang_sutawika_eleutherailm-evaluation-harness_2024,
	title = {{EleutherAI}/lm-evaluation-harness: v0.4.2},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://zenodo.org/doi/10.5281/zenodo.5371628},
	shorttitle = {{EleutherAI}/lm-evaluation-harness},
	abstract = {lm-eval v0.4.2 Release Notes

We are releasing a new minor version of lm-eval for {PyPI} users! We've been very happy to see continued usage of the lm-evaluation-harness, including as a standard testbench to propel new architecture design (https://arxiv.org/abs/2402.18668), to ease new benchmark creation (https://arxiv.org/abs/2402.11548, https://arxiv.org/abs/2402.00786, https://arxiv.org/abs/2403.01469), enabling controlled experimentation on {LLM} evaluation (https://arxiv.org/abs/2402.01781), and more!

New Additions



Request Caching by @inf3rnus - speedups on startup via caching the construction of documents/requests' contexts

Weights and Biases logging by @ayulockin - evals can now be logged to both {WandB} and Zeno!

New Tasks


{KMMLU}, a localized - not (auto) translated! - dataset for testing Korean knowledge by @h-albert-lee @{guijinSON}

{GPQA} by @uanu2002

French Bench by @{ManuelFay}

{EQ}-Bench by @pbevan1 and @sqrkl

{HAERAE}-Bench, readded by @h-albert-lee

Updates to answer parsing on many generative tasks  ({GSM}8k, {MGSM}, {BBH} zeroshot) by @thinknbtfly!

Okapi (translated) Open {LLM} Leaderboard tasks by @uanu2002 and @giux78

Arabic {MMLU} and {aEXAMS} by @khalil-hennara

And more!



Re-introduction of {TemplateLM} base class for lower-code new {LM} class implementations by @anjor

Run the library with metrics/scoring stage skipped via --predict\_only by @baberabb

Many more miscellaneous improvements by a lot of great contributors!


Backwards Incompatibilities

There were a few breaking changes to lm-eval's general {API} or logic we'd like to highlight:

{TaskManager} {API}

previously, users had to call lm\_eval.tasks.initialize\_tasks() to register the library's default tasks, or lm\_eval.tasks.include\_path() to include a custom directory of task {YAML} configs.

Old usage:

import lm\_eval

lm\_eval.tasks.initialize\_tasks() 
\# or:
lm\_eval.tasks.include\_path("/path/to/my/custom/tasks")

 
lm\_eval.simple\_evaluate(model=lm, tasks=["arc\_easy"])


New intended usage:

import lm\_eval

\# optional--only need to instantiate separately if you want to pass custom path!
task\_manager = {TaskManager}() \# pass include\_path="/path/to/my/custom/tasks" if desired

lm\_eval.simple\_evaluate(model=lm, tasks=["arc\_easy"], task\_manager=task\_manager)


get\_task\_dict() now also optionally takes a {TaskManager} object, when wanting to load custom tasks.

This should allow for much faster library startup times due to lazily loading requested tasks or groups.

Updated Stderr Aggregation

Previous versions of the library incorrectly reported erroneously large stderr scores for groups of tasks such as {MMLU}.

We've since updated the formula to correctly aggregate Standard Error scores for groups of tasks reporting accuracies aggregated via their mean across the dataset -- see \#1390 \#1427 for more information.

As always, please feel free to give us feedback or request new features! We're grateful for the community's support.

What's Changed



Add support for {RWKV} models with World tokenizer by @{PicoCreator} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1374

add bypass metric by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1156

Expand docs, update {CITATION}.bib by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1227

Hf: minor egde cases by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1380

Enable override of printed n-shot in table by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1379

Faster Task and Group Loading, Allow Recursive Groups by @lintangsutawika in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1321

Fix for https://github.com/{EleutherAI}/lm-evaluation-harness/issues/1383 by @pminervini in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1384

fix on --task list by @lintangsutawika in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1387

Support for Inf2 optimum class [{WIP}] by @michaelfeil in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1364

Update {README}.md by @mycoalchen in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1398

Fix confusing write\_out.py instructions in {README} by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1371

Use Pooled rather than Combined Variance for calculating stderr of task groupings by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1390

adding hf\_transfer by @michaelfeil in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1400

batch\_size with auto defaults to 1 if No executable batch size found is raised by @pminervini in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1405

Fix printing bug in \#1390 by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1414

Fixes https://github.com/{EleutherAI}/lm-evaluation-harness/issues/1416 by @pminervini in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1418

Fix watchdog timeout by @{JeevanBhoot} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1404

Evaluate by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1385

Add multilingual {ARC} task by @uanu2002 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1419

Add multilingual {TruthfulQA} task by @uanu2002 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1420

[m\_mmul] added multilingual evaluation from alexandrainst/m\_mmlu by @giux78 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1358

Added seeds to evaluator.simple\_evaluate signature by @Am1n3e in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1412

Fix: task weighting by subtask size ; update Pooled Stderr formula slightly by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1427

Refactor utilities into a separate model utils file. by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1429

Nit fix: Updated {OpenBookQA} Readme by @adavidho in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1430

improve hf\_transfer activation by @michaelfeil in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1438

Correct typo in task name in {ARC} documentation by @larekrow in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1443

update bbh, gsm8k, mmlu parsing logic and prompts (Orca2 bbh\_cot\_zeroshot 0\% -{\textgreater} 42\%) by @thnkinbtfly in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1356

Add a new task {HaeRae}-Bench by @h-albert-lee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1445

Group reqs by context by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1425

Add a new task {GPQA} (the part without {CoT}) by @uanu2002 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1434

Added {KMMLU} evaluation method and changed {ReadMe} by @h-albert-lee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1447

Add {TemplateLM} boilerplate {LM} class by @anjor in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1279

Log which subtasks were called with which groups by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1456

{PR} fixing the issue \#1391 (wrong contexts in the mgsm task) by @leocnj in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1440

feat: Add Weights and Biases support by @ayulockin in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1339

Fixed generation args issue affection {OpenAI} completion model by @Am1n3e in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1458

update parsing logic of mgsm following gsm8k (mgsm en 0 -{\textgreater} 50\%) by @thnkinbtfly in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1462

Adding documentation for Weights and Biases {CLI} interface by @veekaybee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1466

Add environment and transformers version logging in results dump by @{LSinev} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1464

Apply code autoformatting with Ruff to tasks/*.py an *init.py by @{LSinev} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1469

Setting trust\_remote\_code to True for {HuggingFace} datasets compatibility by @veekaybee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1467

add arabic mmlu by @khalil-Hennara in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1402

Add Gemma support (Add flag to control {BOS} token usage) by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1465

Revert "Setting trust\_remote\_code to True for {HuggingFace} datasets compatibility" by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1474

Create a means for caching task registration and request building. Ad… by @inf3rnus in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1372

Cont metrics by @lintangsutawika in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1475

Refactor evaluater.evaluate by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1441

add multilingual mmlu eval by @jordane95 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1484

Update {TruthfulQA} val split name by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1488

Fix {AttributeError} in huggingface.py When 'model\_type' is Missing by @richwardle in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1489

Fix duplicated kwargs in some model init by @lchu-ibm in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1495

Add multilingual truthfulqa targets by @jordane95 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1499

Always include {EOS} token as stop sequence by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1480

Improve data-parallel request partitioning for {VLLM} by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1477

modify {WandbLogger} to accept arbitrary kwargs by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1491

Vllm update {DP}+{TP} by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1508

Setting trust\_remote\_code to True for {HuggingFace} datasets compatibility by @veekaybee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1487

Cleaning up unused unit tests by @veekaybee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1516

French Bench by @{ManuelFay} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1500

Hotfix: fix {TypeError} in --trust\_remote\_code by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1517

Fix minor edge cases (\#951 \#1503) by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1520

Openllm benchmark by @baberabb in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1526

Add a new task {GPQA} (the part  {CoT} and generative) by @uanu2002 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1482

Add {EQ}-Bench as per \#1459 by @pbevan1 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1511

Add {WMDP} Multiple-choice by @justinphan3110 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1534

Adding new task : {KorMedMCQA} by @sean0042 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1530

Update docs on {LM}.loglikelihood\_rolling abstract method by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1532

Minor {KMMLU} cleanup by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1502

Cleanup and fixes (Task, Instance, and a little bit of *evaluate) by @{LSinev} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1533

Update installation commands in openai\_completions.py and contributing document and, update wandb\_args description by @naem1023 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1536

Add compatibility for {vLLM}'s new Logprob object by @Yard1 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1549

Fix incorrect max\_gen\_toks generation kwarg default in code2\_text. by @cosmo3769 in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1551

Support jinja templating for task descriptions by @{HishamYahya} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1553

Fix incorrect max\_gen\_toks generation kwarg default in generative Bigbench by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1546

Hardcode {IFEval} to 0-shot by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1506

add Arabic {EXAMS} benchmark by @khalil-Hennara in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1498

{AGIEval} by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1359

cli\_evaluate calls simple\_evaluate with the same verbosity. by @Wongboo in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1563

add manual tqdm disabling management by @artemorloff in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1569

Fix {README} section on vllm integration by @eitanturok in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1579

Fix Jinja template for Advanced {AI} Risk by @{RylanSchaeffer} in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1587

Proposed approach for testing {CLI} arg parsing by @veekaybee in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1566

Patch for Seq2Seq Model predictions by @lintangsutawika in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1584

Add start date in results.json by @djstrong in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1592

Cleanup for v0.4.2 release by @haileyschoelkopf in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1573

Fix eval\_logger import for mmlu/\_generate\_configs.py by @noufmitla in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1593


New Contributors



@{PicoCreator} made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1374

@michaelfeil made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1364

@mycoalchen made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1398

@{JeevanBhoot} made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1404

@uanu2002 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1419

@giux78 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1358

@Am1n3e made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1412

@adavidho made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1430

@larekrow made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1443

@leocnj made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1440

@ayulockin made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1339

@khalil-Hennara made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1402

@inf3rnus made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1372

@jordane95 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1484

@richwardle made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1489

@lchu-ibm made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1495

@pbevan1 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1511

@justinphan3110 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1534

@sean0042 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1530

@naem1023 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1536

@Yard1 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1549

@cosmo3769 made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1551

@{HishamYahya} made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1553

@Wongboo made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1563

@artemorloff made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1569

@eitanturok made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1579

@{RylanSchaeffer} made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1587

@noufmitla made their first contribution in https://github.com/{EleutherAI}/lm-evaluation-harness/pull/1593


Full Changelog: https://github.com/{EleutherAI}/lm-evaluation-harness/compare/v0.4.1...v0.4.2},
	version = {v0.4.2},
	publisher = {[object Object]},
	author = {Lintang Sutawika and Hailey Schoelkopf and Leo Gao and Baber Abbasi and Stella Biderman and Jonathan Tow and ben fattori and Charles Lovering and farzanehnakhaee70 and Jason Phang and Anish Thite and Fazz and Thomas Wang and Niklas Muennighoff and Aflah and sdtblck and nopperl and gakada and tttyuntian and researcher2 and Chris and Julen Etxaniz and Hanwool Albert Lee and Zdeněk Kasner and Khalid and Jeffrey Hsu and Anjor Kanekar and Pawan Sasanka Ammanamanchi and Vicki Boykis and {AndyZwei}},
	urldate = {2024-04-11},
	date = {2024-03-18},
	doi = {10.5281/ZENODO.5371628},
}

@article{faichuk2023war,
	title = {War memes: language transformations after the Russian invasion of Ukraine},
	volume = {12},
	pages = {263--270},
	number = {71},
	journaltitle = {Amazonia Investiga},
	author = {Faichuk, Tetiana and Myroshnichenko, Ilona and Vakulych, Mariia and Fihol, Nadija and Stohnii, Iryna},
	date = {2023},
}

@misc{chia_instructeval_2023,
	title = {{INSTRUCTEVAL}: Towards Holistic Evaluation of Instruction-Tuned Large Language Models},
	url = {http://arxiv.org/abs/2306.04757},
	shorttitle = {{INSTRUCTEVAL}},
	abstract = {Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as {GPT}-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present {INSTRUCTEVAL}, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and training methods. Our findings reveal that the quality of instruction data is the most crucial factor in scaling model performance. While open-source models demonstrate impressive writing abilities, there is substantial room for improvement in problem-solving and alignment. We are encouraged by the rapid development of models by the open-source community, but we also highlight the need for rigorous evaluation to support claims made about these models. Through {INSTRUCTEVAL}, we aim to foster a deeper understanding of instruction-tuned models and advancements in their capabilities. {INSTRUCTEVAL} is publicly available at https://github.com/declare-lab/instruct-eval.},
	number = {{arXiv}:2306.04757},
	publisher = {{arXiv}},
	author = {Chia, Yew Ken and Hong, Pengfei and Bing, Lidong and Poria, Soujanya},
	urldate = {2024-04-10},
	date = {2023-06-15},
	eprinttype = {arxiv},
	eprint = {2306.04757 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhang_instruction_2024,
	title = {Instruction Tuning for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2308.10792},
	shorttitle = {Instruction Tuning for Large Language Models},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning ({IT}), a crucial technique to enhance the capabilities and controllability of large language models ({LLMs}). Instruction tuning refers to the process of further training {LLMs} on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of {LLMs} and the users' objective of having {LLMs} adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of {IT}, the construction of {IT} datasets, the training of {IT} models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of {IT} (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of {IT} along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	number = {{arXiv}:2308.10792},
	publisher = {{arXiv}},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	urldate = {2024-04-10},
	date = {2024-03-13},
	eprinttype = {arxiv},
	eprint = {2308.10792 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-judge with {MT}-Bench and Chatbot Arena},
	url = {http://arxiv.org/abs/2306.05685},
	abstract = {Evaluating large language model ({LLM}) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong {LLMs} as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of {LLM}-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between {LLM} judges and human preferences by introducing two benchmarks: {MT}-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong {LLM} judges like {GPT}-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, {LLM}-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of {LLaMA} and Vicuna. We will publicly release {MT}-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
	number = {{arXiv}:2306.05685},
	publisher = {{arXiv}},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2023-09-28},
	date = {2023-07-11},
	eprinttype = {arxiv},
	eprint = {2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, useful},
}

@misc{brockman2023evals,
	title = {Evals are surprisingly often all you need},
	url = {https://twitter.com/gdb/status/1733553161884127435},
	author = {Brockman, Greg},
	date = {2023-12-09},
}

@article{cowley_framework_2022,
	title = {A framework for rigorous evaluation of human performance in human and machine learning comparison studies},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-08078-3},
	doi = {10.1038/s41598-022-08078-3},
	abstract = {Abstract
            Rigorous comparisons of human and machine learning algorithm performance on the same task help to support accurate claims about algorithm success rates and advances understanding of their performance relative to that of human performers. In turn, these comparisons are critical for supporting advances in artificial intelligence. However, the machine learning community has lacked a standardized, consensus framework for performing the evaluations of human performance necessary for comparison. We demonstrate common pitfalls in a designing the human performance evaluation and propose a framework for the evaluation of human performance, illustrating guiding principles for a successful comparison. These principles are first, to design the human evaluation with an understanding of the differences between human and algorithm cognition; second, to match trials between human participants and the algorithm evaluation, and third, to employ best practices for psychology research studies, such as the collection and analysis of supplementary and subjective data and adhering to ethical review protocols. We demonstrate our framework’s utility for designing a study to evaluate human performance on a one-shot learning task. Adoption of this common framework may provide a standard approach to evaluate algorithm performance and aid in the reproducibility of comparisons between human and machine learning algorithm performance.},
	pages = {5444},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Cowley, Hannah P. and Natter, Mandy and Gray-Roncal, Karla and Rhodes, Rebecca E. and Johnson, Erik C. and Drenkow, Nathan and Shead, Timothy M. and Chance, Frances S. and Wester, Brock and Gray-Roncal, William},
	urldate = {2024-04-10},
	date = {2022-03-31},
	langid = {english},
	keywords = {baseline, human-baseline, useful},
}

@article{radford2019language,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	pages = {9},
	number = {8},
	journaltitle = {{OpenAI} blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and {others}},
	date = {2019},
}

@misc{gpt3,
	title = {Language models are few-shot learners},
	url = {https://arxiv.org/abs/2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	date = {2020},
	doi = {10.48550/ARXIV.2005.14165},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, useful},
}

@misc{trinh_simple_2019,
	title = {A Simple Method for Commonsense Reasoning},
	url = {http://arxiv.org/abs/1806.02847},
	abstract = {Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large {RNN} language models that operate at word or character level on {LM}-1-Billion, {CommonCrawl}, {SQuAD}, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.},
	number = {{arXiv}:1806.02847},
	publisher = {{arXiv}},
	author = {Trinh, Trieu H. and Le, Quoc V.},
	urldate = {2024-04-09},
	date = {2019-09-26},
	eprinttype = {arxiv},
	eprint = {1806.02847 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2024-04-09},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-04-09},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{vaswani2017attention,
	title = {Attention is all you need},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	date = {2017},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2024-04-09},
	date = {2015-05-28},
	langid = {english},
	keywords = {useful},
}

@article{patwardhan_transformers_2023,
	title = {Transformers in the Real World: A Survey on {NLP} Applications},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/14/4/242},
	doi = {10.3390/info14040242},
	shorttitle = {Transformers in the Real World},
	abstract = {The field of Natural Language Processing ({NLP}) has undergone a significant transformation with the introduction of Transformers. From the first introduction of this technology in 2017, the use of transformers has become widespread and has had a profound impact on the field of {NLP}. In this survey, we review the open-access and real-world applications of transformers in {NLP}, specifically focusing on those where text is the primary modality. Our goal is to provide a comprehensive overview of the current state-of-the-art in the use of transformers in {NLP}, highlight their strengths and limitations, and identify future directions for research. In this way, we aim to provide valuable insights for both researchers and practitioners in the field of {NLP}. In addition, we provide a detailed analysis of the various challenges faced in the implementation of transformers in real-world applications, including computational efficiency, interpretability, and ethical considerations. Moreover, we highlight the impact of transformers on the {NLP} community, including their influence on research and the development of new {NLP} models.},
	pages = {242},
	number = {4},
	journaltitle = {Information},
	shortjournal = {Information},
	author = {Patwardhan, Narendra and Marrone, Stefano and Sansone, Carlo},
	urldate = {2024-04-09},
	date = {2023-04-17},
	langid = {english},
}

@article{sergeytsev2022russia,
	title = {What russia should do with ukraine},
	journaltitle = {{RIA} Novosti. {URL}: https://ria. ru/20220403/ukraina-1781469605. html (in Russian)},
	author = {Sergeytsev, Timofey and dolzhna sdelats Ukrainoy, Chto Rossiya},
	date = {2022},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2024-04-08},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{lison2016opensubtitles2016,
	title = {Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
	author = {Lison, Pierre and Tiedemann, Jörg},
	date = {2016},
	note = {Publisher: European Language Resources Association},
}

@misc{robinson_leveraging_2023,
	title = {Leveraging Large Language Models for Multiple Choice Question Answering},
	url = {http://arxiv.org/abs/2210.12353},
	abstract = {While large language models ({LLMs}) like {GPT}-3 have achieved impressive results on multiple choice question answering ({MCQA}) tasks in the zero, one, and few-shot settings, they generally lag behind the {MCQA} state of the art ({SOTA}). {MCQA} tasks have traditionally been presented to {LLMs} like cloze tasks. An {LLM} is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the {LLM} jointly and have it output the symbol (e.g., "A") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the {LLM} it is used with must be able to associate answer options with the symbols that represent them. The {LLM} needs what we term multiple choice symbol binding ({MCSB}) ability. This ability varies greatly by model. We show that a model with high {MCSB} ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the {SOTA}, suggesting that the {MCQA} ability of {LLMs} has been previously underestimated.},
	number = {{arXiv}:2210.12353},
	publisher = {{arXiv}},
	author = {Robinson, Joshua and Rytting, Christopher Michael and Wingate, David},
	urldate = {2024-03-01},
	date = {2023-03-16},
	eprinttype = {arxiv},
	eprint = {2210.12353 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, useful},
}

@article{min_recent_2024,
	title = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3605943},
	doi = {10.1145/3605943},
	shorttitle = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models},
	abstract = {Large, pre-trained language models ({PLMs}) such as {BERT} and {GPT} have drastically changed the Natural Language Processing ({NLP}) field. For numerous {NLP} tasks, approaches leveraging {PLMs} have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate {NLP} tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of {PLM} architectures and a comprehensive view of the shift to {PLM}-driven {NLP} techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses {PLM} limitations and suggested directions for future research.},
	pages = {1--40},
	number = {2},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	urldate = {2024-04-06},
	date = {2024-02-29},
	langid = {english},
	keywords = {useful},
}

@inproceedings{jorgensen_multifin_2023,
	location = {Dubrovnik, Croatia},
	title = {{MultiFin}: A Dataset for Multilingual Financial {NLP}},
	url = {https://aclanthology.org/2023.findings-eacl.66},
	doi = {10.18653/v1/2023.findings-eacl.66},
	shorttitle = {{MultiFin}},
	pages = {894--909},
	booktitle = {Findings of the Association for Computational Linguistics: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Jørgensen, Rasmus and Brandt, Oliver and Hartmann, Mareike and Dai, Xiang and Igel, Christian and Elliott, Desmond},
	urldate = {2023-09-14},
	date = {2023},
	langid = {english},
	keywords = {dataset, important, nlp},
}

@article{markov,
	title = {Markov chains},
	journaltitle = {Springer-Verlag, New York},
	author = {Chung, Kai Lai},
	date = {1967},
	note = {Publisher: Springer},
}

@article{surveyNLP,
	title = {A comprehensive survey on word representation models: From classical to state-of-the-art word representation language models},
	volume = {abs/2010.15036},
	url = {https://arxiv.org/abs/2010.15036},
	journaltitle = {{CoRR}},
	author = {Naseem, Usman and Razzak, Imran and Khan, Shah Khalid and Prasad, Mukesh},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2010.15036},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Tue, 03 Nov 2020 11:44:23 +0100},
}

@misc{stateval,
	title = {Language model evaluation beyond perplexity},
	url = {https://arxiv.org/abs/2106.00085},
	publisher = {{arXiv}},
	author = {Meister, Clara and Cotterell, Ryan},
	date = {2021},
	doi = {10.48550/ARXIV.2106.00085},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
}

@article{shannon,
	title = {Claude shannon and “A mathematical theory of communication”},
	pages = {1--12},
	journaltitle = {Relation},
	author = {Ahammad, Parvez and Daskalakis, Konstantinos and Etesami, Omid and Frome, Andrea},
	date = {2004},
}

@article{chip2019evaluation,
	title = {Evaluation metrics for language modeling},
	url = {https://thegradient.pub/understanding-evaluation-metrics-for-language-models/},
	journaltitle = {The Gradient},
	author = {Huyen, Chip},
	date = {2019},
}

@inproceedings{seganti-etal-2021-multilingual,
	location = {Online},
	title = {Multilingual entity and relation extraction dataset and model},
	url = {https://aclanthology.org/2021.eacl-main.166},
	doi = {10.18653/v1/2021.eacl-main.166},
	abstract = {We present a novel dataset and model for a multilingual setting to approach the task of Joint Entity and Relation Extraction. The {SMiLER} dataset consists of 1.1 M annotated sentences, representing 36 relations, and 14 languages. To the best of our knowledge, this is currently both the largest and the most comprehensive dataset of this type. We introduce {HERBERTa}, a pipeline that combines two independent {BERT} models: one for sequence classification, and the other for entity tagging. The model achieves micro F1 81.49 for English on this dataset, which is close to the current {SOTA} on {CoNLL}, {SpERT}.},
	pages = {1946--1955},
	booktitle = {Proceedings of the 16th conference of the european chapter of the association for computational linguistics: Main volume},
	publisher = {Association for Computational Linguistics},
	author = {Seganti, Alessandro and Firląg, Klaudia and Skowronska, Helena and Satława, Michał and Andruszkiewicz, Piotr},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	date = {2021-04},
}

@misc{roberts_data_2023,
	title = {Data Contamination Through the Lens of Time},
	url = {http://arxiv.org/abs/2310.10628},
	abstract = {Recent claims about the impressive abilities of large language models ({LLMs}) are often supported by evaluating publicly available benchmarks. Since {LLMs} train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in {LLMs} by using the natural experiment of training cutoffs in {GPT} models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among {LLM} pass rate vs. {GitHub} popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of {LLMs} that train on webscale data.},
	number = {{arXiv}:2310.10628},
	publisher = {{arXiv}},
	author = {Roberts, Manley and Thakur, Himanshu and Herlihy, Christine and White, Colin and Dooley, Samuel},
	urldate = {2024-02-21},
	date = {2023-10-16},
	eprinttype = {arxiv},
	eprint = {2310.10628 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{akter_-depth_2023,
	title = {An In-depth Look at Gemini's Language Abilities},
	url = {http://arxiv.org/abs/2312.11444},
	abstract = {The recently released Google Gemini class of models are the first to comprehensively report results that rival the {OpenAI} {GPT} series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the {OpenAI} {GPT} and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding {GPT} 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark},
	number = {{arXiv}:2312.11444},
	publisher = {{arXiv}},
	author = {Akter, Syeda Nahida and Yu, Zichun and Muhamed, Aashiq and Ou, Tianyue and Bäuerle, Alex and Cabrera, Ángel Alexander and Dholakia, Krish and Xiong, Chenyan and Neubig, Graham},
	urldate = {2024-03-04},
	date = {2023-12-24},
	eprinttype = {arxiv},
	eprint = {2312.11444 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{kuchmiichuk-2023-silver,
	location = {Dubrovnik, Croatia},
	title = {Silver data for coreference resolution in Ukrainian: Translation, alignment, and projection},
	url = {https://aclanthology.org/2023.unlp-1.8},
	abstract = {Low-resource languages continue to present challenges for current {NLP} methods, and multilingual {NLP} is gaining attention in the research community. One of the main issues is the lack of sufficient high-quality annotated data for low-resource languages. In this paper, we show how labeled data for high-resource languages such as English can be used in low-resource {NLP}. We present two silver datasets for coreference resolution in Ukrainian, adapted from existing English data by manual translation and machine translation in combination with automatic alignment and annotation projection. The code is made publicly available.},
	pages = {62--72},
	booktitle = {Proceedings of the second ukrainian natural language processing workshop ({UNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kuchmiichuk, Pavlo},
	date = {2023-05},
}

@software{ua_datasets_2021,
	title = {ua$_{\textrm{d}}$atasets: a collection of Ukrainian language datasets},
	url = {https://github.com/fido-ai/ua-datasets},
	version = {0.0.1},
	author = {Ivanyuk-Skulskiy, Bogdan and Zaliznyi, Anton and Reshetar, Oleksand and Protsyk, Oleksiy and Romanchuk, Bohdan and Shpihanovych, Vladyslav},
	date = {2021-10},
}

@inproceedings{hamotskyi-etal-2024-fincorpus,
	location = {Torino, Italy},
	title = {{FinCorpus}-{DE}10k: A corpus for the german financial domain},
	abstract = {We introduce a predominantly German corpus comprising 12.5k {PDF} documents sourced from the financial domain. The corresponding extracted textual data encompasses more than 165 million tokens derived predominantly from German, and to a lesser extent, bilingual documents. 
We provide detailed information about the document types included in the corpus, such as final terms, base prospectuses, annual reports, information materials, law documents, international financial reporting standards, and monthly reports from the Bundesbank, accompanied by comprehensive statistical analysis.
To our knowledge, it is the first non-email German financial corpus available, and we hope it will fill this gap and foster further research in the financial domain both in the German language and in multilingual contexts.},
	booktitle = {the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation ({LREC}-{COLING} 2024)},
	publisher = {European Language Resources Association},
	author = {Hamotskyi, Serhii and Kozaeva, Nata and Hänig, Christian},
	date = {2024-05},
}

@misc{costa_domain_2022,
	title = {Domain Adaptation in Neural Machine Translation using a Qualia-Enriched {FrameNet}},
	url = {http://arxiv.org/abs/2202.10287},
	abstract = {In this paper we present Scylla, a methodology for domain adaptation of Neural Machine Translation ({NMT}) systems that make use of a multilingual {FrameNet} enriched with qualia relations as an external knowledge base. Domain adaptation techniques used in {NMT} usually require fine-tuning and in-domain training data, which may pose difficulties for those working with lesser-resourced languages and may also lead to performance decay of the {NMT} system for out-of-domain sentences. Scylla does not require fine-tuning of the {NMT} model, avoiding the risk of model over-fitting and consequent decrease in performance for out-of-domain translations. Two versions of Scylla are presented: one using the source sentence as input, and another one using the target sentence. We evaluate Scylla in comparison to a state-of-the-art commercial {NMT} system in an experiment in which 50 sentences from the Sports domain are translated from Brazilian Portuguese to English. The two versions of Scylla significantly outperform the baseline commercial system in {HTER}.},
	number = {{arXiv}:2202.10287},
	publisher = {{arXiv}},
	author = {Costa, Alexandre Diniz and Marim, Mateus Coutinho and Matos, Ely Edison da Silva and Torrent, Tiago Timponi},
	urldate = {2024-03-22},
	date = {2022-02-21},
	eprinttype = {arxiv},
	eprint = {2202.10287 [cs]},
	keywords = {Computer Science - Computation and Language, E.1},
}

@book{1817oeuvres,
	title = {Oeuvres complètes de Voltaire},
	url = {https://books.google.com.ua/books?id=Lh8TAAAAQAAJ},
	series = {Oeuvres complètes de Voltaire},
	publisher = {chez Th. Desoer},
	date = {1817},
	note = {Number: Bd. 5,Teil 1},
}

@misc{linTruthfulQAMeasuringHow2022,
	title = {{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
	url = {http://arxiv.org/abs/2109.07958},
	shorttitle = {{TruthfulQA}},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested {GPT}-3, {GPT}-Neo/J, {GPT}-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other {NLP} tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	number = {{arXiv}:2109.07958},
	publisher = {{arXiv}},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	urldate = {2023-10-02},
	date = {2022-05-07},
	eprinttype = {arxiv},
	eprint = {2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, benchmark-task, toread, useful},
}

@inproceedings{labaContextualEmbeddingsUkrainian2023,
	location = {Dubrovnik, Croatia},
	title = {Contextual Embeddings for Ukrainian: A Large Language Model Approach to Word Sense Disambiguation},
	url = {https://aclanthology.org/2023.unlp-1.2},
	doi = {10.18653/v1/2023.unlp-1.2},
	shorttitle = {Contextual Embeddings for Ukrainian},
	eventtitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	pages = {11--19},
	booktitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Laba, Yurii and Mudryi, Volodymyr and Chaplynskyi, Dmytro and Romanyshyn, Mariana and Dobosevych, Oles},
	urldate = {2023-09-28},
	date = {2023},
	langid = {english},
	keywords = {benchmark-task, important, ua},
}

@misc{Sira2019,
	title = {Towards an automatic recognition of mixed languages: The Ukrainian-Russian hybrid language Surzhyk},
	url = {http://arxiv.org/abs/1912.08582},
	shorttitle = {Towards an automatic recognition of mixed languages},
	abstract = {Language interference is common in today's multilingual societies where more languages are being in contact and as a global final result leads to the creation of hybrid languages. These, together with doubts on their right to be officially recognised made emerge in the area of computational linguistics the problem of their automatic identification and further elaboration. In this paper, we propose a first attempt to identify the elements of a Ukrainian-Russian hybrid language, Surzhyk, through the adoption of the example-based rules created with the instruments of programming language R. Our example-based study consists of: 1) analysis of spoken samples of Surzhyk registered by Del Gaudio (2010) in Kyiv area and creation of the written corpus; 2) production of specific rules on the identification of Surzhyk patterns and their implementation; 3) testing the code and analysing the effectiveness.},
	number = {{arXiv}:1912.08582},
	publisher = {{arXiv}},
	author = {Sira, Nataliya and Di Nunzio, Giorgio Maria and Nosilia, Viviana},
	urldate = {2024-01-16},
	date = {2019-12-18},
	eprinttype = {arxiv},
	eprint = {1912.08582 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries, useful},
}

@article{Matveyeva2017,
	title = {Modern language situation (on the basis of the 2017 survey)},
	volume = {0},
	issn = {2522-9281},
	url = {http://lcmp.ukma.edu.ua/article/view/123368},
	doi = {10.18523/lcmp2522-92812017123368},
	number = {3},
	journaltitle = {Language: classic - modern - postmodern},
	shortjournal = {{LCMP}},
	author = {Matveyeva, Nataliya},
	urldate = {2024-01-16},
	date = {2017-11-02},
	keywords = {ua, useful},
}

@article{Racek2024,
	title = {The Russian war in Ukraine increased Ukrainian language use on social media},
	volume = {2},
	issn = {2731-9121},
	url = {https://www.nature.com/articles/s44271-023-00045-6},
	doi = {10.1038/s44271-023-00045-6},
	abstract = {Abstract
            The use of language is innately political, often a vehicle of cultural identity and the basis for nation building. Here, we examine language choice and tweeting activity of Ukrainian citizens based on 4,453,341 geo-tagged tweets from 62,712 users before and during the Russian war in Ukraine, from January 2020 to October 2022. Using statistical models, we disentangle sample effects, arising from the in- and outflux of users on Twitter (now X), from behavioural effects, arising from behavioural changes of the users. We observe a steady shift from the Russian language towards Ukrainian already before the war, which drastically speeds up with its outbreak. We attribute these shifts in large part to users’ behavioural changes. Notably, our analysis shows that more than half of the Russian-tweeting users switch towards Ukrainian with the Russian invasion. We interpret these findings as users’ conscious choice towards a more Ukrainian (online) identity and self-definition of being Ukrainian.},
	pages = {1},
	number = {1},
	journaltitle = {Communications Psychology},
	shortjournal = {Commun Psychol},
	author = {Racek, Daniel and Davidson, Brittany I. and Thurner, Paul W. and Zhu, Xiao Xiang and Kauermann, Göran},
	urldate = {2024-01-16},
	date = {2024-01-10},
	langid = {english},
	keywords = {ua, ukr, useful},
}

@inproceedings{Kanishcheva2023,
	location = {Dubrovnik, Croatia},
	title = {The Parliamentary Code-Switching Corpus: Bilingualism in the Ukrainian Parliament in the 1990s-2020s},
	url = {https://aclanthology.org/2023.unlp-1.10},
	doi = {10.18653/v1/2023.unlp-1.10},
	shorttitle = {The Parliamentary Code-Switching Corpus},
	eventtitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	pages = {79--90},
	booktitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kanishcheva, Olha and Kovalova, Tetiana and Shvedova, Maria and Von Waldenfels, Ruprecht},
	urldate = {2023-12-10},
	date = {2023},
	langid = {english},
}

@incollection{Korobov,
	title = {Morphological analyzer and generator for russian and ukrainian languages},
	volume = {542},
	isbn = {978-3-319-26122-5},
	url = {http://dx.doi.org/10.1007/978-3-319-26123-2_31},
	series = {Communications in computer and information science},
	pages = {320--332},
	booktitle = {Analysis of images, social networks and texts},
	publisher = {Springer International Publishing},
	author = {Korobov, Mikhail},
	editor = {Khachay, Mikhail Yu. and Konstantinova, Natalia and Panchenko, Alexander and Ignatov, Dmitry I. and Labunets, Valeri G.},
	doi = {10.1007/978-3-319-26123-2_31},
	keywords = {{LanguageTool}, Morphological analyzer, Morphological generator, Open source, {OpenCorpora}, Russian, Ukrainian, pymorphy, pymorphy2},
}

@misc{Korobov2015,
	title = {Morphological Analyzer and Generator for Russian and Ukrainian Languages},
	url = {http://arxiv.org/abs/1503.07283},
	abstract = {pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian languages. It uses large efficiently encoded lexi- cons built from {OpenCorpora} and {LanguageTool} data. A set of linguistically motivated rules is developed to enable morphological analysis and generation of out-of-vocabulary words observed in real-world documents. For Russian pymorphy2 provides state-of-the-arts morphological analysis quality. The analyzer is implemented in Python programming language with optional C++ extensions. Emphasis is put on ease of use, documentation and extensibility. The package is distributed under a permissive open-source license, encouraging its use in both academic and commercial setting.},
	number = {{arXiv}:1503.07283},
	publisher = {{arXiv}},
	author = {Korobov, Mikhail},
	urldate = {2023-10-24},
	date = {2015-03-25},
	eprinttype = {arxiv},
	eprint = {1503.07283 [cs]},
	keywords = {Computer Science - Computation and Language, tool, useful},
}

@inproceedings{syvokonUNLP2023Shared2023,
	location = {Dubrovnik, Croatia},
	title = {The {UNLP} 2023 Shared Task on Grammatical Error Correction for Ukrainian},
	url = {https://aclanthology.org/2023.unlp-1.16},
	doi = {10.18653/v1/2023.unlp-1.16},
	eventtitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	pages = {132--137},
	booktitle = {Proceedings of the Second Ukrainian Natural Language Processing Workshop ({UNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Syvokon, Oleksiy and Romanyshyn, Mariana},
	urldate = {2023-09-28},
	date = {2023},
	langid = {english},
	keywords = {benchmark-task, important, ua},
}

@misc{Syvokon2022,
	title = {{UA}-{GEC}: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language},
	url = {http://arxiv.org/abs/2103.16997},
	shorttitle = {{UA}-{GEC}},
	abstract = {We present a corpus professionally annotated for grammatical error correction ({GEC}) and fluency edits in the Ukrainian language. To the best of our knowledge, this is the first {GEC} corpus for the Ukrainian language. We collected texts with errors (20,715 sentences) from a diverse pool of contributors, including both native and non-native speakers. The data cover a wide variety of writing domains, from text chats and essays to formal writing. Professional proofreaders corrected and annotated the corpus for errors relating to fluency, grammar, punctuation, and spelling. This corpus can be used for developing and evaluating {GEC} systems in Ukrainian. More generally, it can be used for researching multilingual and low-resource {NLP}, morphologically rich languages, document-level {GEC}, and fluency correction. The corpus is publicly available at https://github.com/grammarly/ua-gec},
	number = {{arXiv}:2103.16997},
	publisher = {{arXiv}},
	author = {Syvokon, Oleksiy and Nahorna, Olena},
	urldate = {2023-10-11},
	date = {2022-11-08},
	eprinttype = {arxiv},
	eprint = {2103.16997 [cs]},
	keywords = {Computer Science - Computation and Language, corpus, dataset, ua, useful},
}

@inproceedings{inkpen2005automatic,
	title = {Automatic identification of cognates and false friends in French and English},
	volume = {9},
	pages = {251--257},
	booktitle = {Proceedings of the international conference recent advances in natural language processing},
	author = {Inkpen, Diana and Frunza, Oana and Kondrak, Grzegorz},
	date = {2005},
}

@inproceedings{babych2019unsupervised,
	title = {Unsupervised induction of ukrainian morphological paradigms for the new lexicon: Extending coverage for named entities and neologisms using inflection tables and unannotated corpora},
	pages = {1--11},
	booktitle = {Proceedings of the 7th workshop on balto-slavic natural language processing},
	author = {Babych, Bogdan},
	date = {2019},
}

@article{rehbein2014check,
	title = {How to check understanding across languages. An introduction into the Pragmatic Index of Language Distance ({PILaD}) usable to measure mutual understanding in receptive multilingualism, illustrated by conversations in Russian, Ukrainian and Polish},
	volume = {5},
	pages = {131--171},
	number = {1},
	journaltitle = {Applied Linguistics Review},
	author = {Rehbein, Jochen and Romaniuk, Olena},
	date = {2014},
	note = {Publisher: De Gruyter Mouton},
}

@article{grenoble2010contact,
	title = {Contact and the development of the Slavic languages},
	pages = {581--597},
	journaltitle = {The handbook of language contact},
	author = {Grenoble, Lenore A},
	date = {2010},
	note = {Publisher: Wiley Online Library},
	keywords = {useful},
}

@article{danylyuk2022main,
	title = {The main features of the ukrainian grammar},
	author = {Danylyuk, Nina and Masytska, Tetiana and O’Brien, Douglas and Rohach, Oksana},
	date = {2022},
	note = {Publisher: Волинський національний університет імені Лесі Українки},
}

@book{press2015ukrainian,
	title = {Ukrainian: A comprehensive grammar},
	publisher = {Routledge},
	author = {Press, Ian and Pugh, Stefan},
	date = {2015},
}

@article{5c48fce9-c05d-3d4e-94c1-cd6079bff660,
	title = {The language question in the ukraine in the twentieth century (1900-1941)},
	volume = {11},
	issn = {03635570},
	url = {http://www.jstor.org/stable/41036243},
	pages = {118--224},
	number = {1},
	journaltitle = {Harvard Ukrainian Studies},
	author = {{SHEVELOV}, {GEORGE} Y.},
	urldate = {2024-01-16},
	date = {1987},
	note = {Publisher: [President and Fellows of Harvard College, Harvard Ukrainian Research Institute]},
}

@article{karunyk2017ukrainian,
	title = {The ukrainian spelling reforms, half-reforms, non-reforms and anti-reforms as manifestation of the soviet language policy},
	volume = {14},
	pages = {91--110},
	number = {1},
	journaltitle = {Studi Slavistici},
	author = {Karunyk, Kateryna},
	date = {2017},
	note = {Publisher: Firenze University Press},
}

@article{1ad9e7d5-c0eb-33df-ae6c-1fdbd2549d75,
	title = {The executed renaissance paradigm revisited},
	volume = {27},
	issn = {03635570},
	url = {http://www.jstor.org/stable/41036862},
	pages = {67--96},
	number = {1},
	journaltitle = {Harvard Ukrainian Studies},
	author = {{HRYN}, {HALYNA}},
	urldate = {2024-01-16},
	date = {2004},
	note = {Publisher: [President and Fellows of Harvard College, Harvard Ukrainian Research Institute]},
}

@article{remy2017despite,
	title = {Despite the valuev directive: Books permitted by the censors in violation of the restrictions against ukrainian publishing, 1864-1904},
	volume = {4},
	pages = {113--129},
	number = {2},
	journaltitle = {East/West: Journal of Ukrainian Studies ({EWJUS})},
	author = {Remy, Johannes and {others}},
	date = {2017},
	note = {Publisher: Canadian Institute of Ukrainian Studies-University of Alberta},
}

@misc{enwikisource:13111073,
	title = {Translation:Valuyev circular — Wikisource,},
	url = {https://en.wikisource.org/w/index.php?title=Translation:Valuyev_Circular&oldid=13111073},
	author = {{Wikisource}},
	date = {2023},
}

@article{dibrova2017valuev,
	title = {The valuev circular and the end of little russian literature},
	pages = {123--138},
	number = {4},
	journaltitle = {Kyiv-Mohyla Humanities Journal},
	author = {Dibrova, Volodymyr},
	date = {2017},
}

@article{doi:10.1016/j.euras.2014.05.005,
	title = {Ukraine and russia: Legacies of the imperial past and competing memories},
	volume = {5},
	url = {https://doi.org/10.1016/j.euras.2014.05.005},
	doi = {10.1016/j.euras.2014.05.005},
	abstract = {The legacy of the tsarist Empire and the Soviet Union is one of the crucial factors for an understanding and an explanation of current affairs in the post-Soviet space. This is especially true for Ukraine and for Russian–Ukrainian relations. Russia regards Ukraine as a part of its own strategic orbit, while many Ukrainians want to liberate themselves from the Russian hegemony and advocate a closer cooperation with the European Union. This controversy culminated in late 2013, when Russian pressure led to a re-orientation of Ukrainian policy and a rapprochement with Russia. In this paper I present some reflections on the significance of the imperial heritage for the Russian–Ukrainian relationship. I analyse the different discourses and the Ukrainian and Russian historical narratives, politics of history and competing memories. The Russian–Ukrainian relationship was and is still characterized by an obvious asymmetry, a hegemony of Russia over Ukraine. Russia uses the Orthodox Church and the traditional dominance of the Russian language as instruments for its policy. Not only Russian historians, but also politicians and even the Russian President try to impose the imperial narrative on Ukraine. They are supported by a significant part of Ukrainians, who adhere to the ideal of a common Russia-led Orthodox East Slavic world. Other Ukrainian historians and politicians use the Ukrainian language and the Ukrainian historical narrative with its national myths of liberty and of Ukraine's closeness to Europe in their struggle against the Russian hegemony. The on-going “War of memories” is of special interest. Both sides use and abuse history as a political weapon, and the controversies about the heritage of Kievan Rus’, the interpretation of Mazepa, the Holodomor and {WW} {II} are not only academic, but also political issues. They reflect the struggle over the geopolitical and cultural orientation of Ukraine which is of crucial importance for the future development of the post-Soviet space and of Eastern Europe.},
	pages = {107--115},
	number = {2},
	journaltitle = {Journal of Eurasian Studies},
	author = {Kappeler, Andreas},
	date = {2014},
	note = {tex.eprint: https://doi.org/10.1016/j.euras.2014.05.005},
}

@misc{newlinesmagMotherTongue,
	title = {Mother Tongue: The Story of a Ukrainian Language Convert — newlinesmag.com},
	url = {https://newlinesmag.com/first-person/mother-tongue-the-story-of-a-ukrainian-language-convert/},
	date = {2023},
	keywords = {not-a-paper},
}

@misc{ratinggroupSixthNational,
	title = {The sixth national poll: The language issue in Ukraine (March 19th, 2022) — ratinggroup.ua},
	url = {https://ratinggroup.ua/en/research/ukraine/language_issue_in_ukraine_march_19th_2022.html},
	date = {2022},
}

@article{bernsand2001surzhyk,
	title = {Surzhyk and national identity in Ukrainian nationalist language ideology},
	volume = {17},
	pages = {38--47},
	number = {2001},
	journaltitle = {Berliner Osteuropa Info},
	author = {Bernsand, Niklas},
	date = {2001},
}

@inproceedings{hentschel2020ukrainisch,
	title = {Ukrainisch-russisches und russisch-ukrainisches Code-Mixing. Untersuchungen in drei Regionen im Süden der Ukraine},
	pages = {105--132},
	booktitle = {Colloquium: New philologies},
	author = {Hentschel, Gerd and Reuther, Tilmann},
	date = {2020},
	keywords = {useful},
}

@book{1130282272476965120,
	title = {Keeping a record : literary purges in Soviet Ukraine (1930s), a bio-bibliography},
	url = {https://cir.nii.ac.jp/crid/1130282272476965120},
	series = {Occasional research reports},
	publisher = {Canadian Institute of Ukrainian Studies, University of Alberta in association with Ukrainian Famine Research Centre, Toronto, Ont.},
	author = {Luckyj, George Stephen Nestor},
	date = {1987},
}

@book{krawchenko1987social,
	title = {Social change and national consciousness in twentieth-century Ukraine},
	publisher = {{CIUS} Press},
	author = {Krawchenko, Bohdan},
	date = {1987},
}

@article{kulyk2018shedding,
	title = {Shedding Russianness, recasting Ukrainianness: The post-Euromaidan dynamics of ethnonational identifications in Ukraine},
	volume = {34},
	pages = {119--138},
	number = {2},
	journaltitle = {Post-Soviet Affairs},
	author = {Kulyk, Volodymyr},
	date = {2018},
	note = {Publisher: Taylor \& Francis},
	keywords = {useful},
}

@article{marshall2002post,
	title = {Post-Soviet language policy and the language utilization patterns of Kyivan youth},
	volume = {1},
	pages = {237--260},
	journaltitle = {Language Policy Series},
	shortjournal = {Language Policy},
	author = {Marshall, Camelot Ann},
	date = {2002},
	note = {Publisher: Springer},
	keywords = {ua, useful},
}

@misc{noauthor_notitle_nodate,
}

@misc{taskCBT,
	title = {The goldilocks principle: Reading children's books with explicit memory representations},
	url = {https://arxiv.org/abs/1511.02301},
	publisher = {{arXiv}},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	date = {2015},
	doi = {10.48550/ARXIV.1511.02301},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, useful},
}

@misc{sakaguchi_winogrande_2019,
	title = {{WinoGrande}: An Adversarial Winograd Schema Challenge at Scale},
	url = {http://arxiv.org/abs/1907.10641},
	shorttitle = {{WinoGrande}},
	abstract = {The Winograd Schema Challenge ({WSC}) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of {WSC}. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce {WinoGrande}, a large-scale dataset of 44k problems, inspired by the original {WSC} design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel {AfLite} algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on {WinoGrande} achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - {WSC} (90.1\%), {DPR} (93.1\%), {COPA} (90.6\%), {KnowRef} (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of {WinoGrande} when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
	number = {{arXiv}:1907.10641},
	publisher = {{arXiv}},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	urldate = {2024-01-04},
	date = {2019-11-21},
	eprinttype = {arxiv},
	eprint = {1907.10641 [cs]},
	keywords = {Computer Science - Computation and Language, todo},
}

@inproceedings{stanovsky_evaluating_2019,
	location = {Florence, Italy},
	title = {Evaluating Gender Bias in Machine Translation},
	url = {https://www.aclweb.org/anthology/P19-1164},
	doi = {10.18653/v1/P19-1164},
	eventtitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	pages = {1679--1684},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Stanovsky, Gabriel and Smith, Noah A. and Zettlemoyer, Luke},
	urldate = {2024-01-04},
	date = {2019},
	langid = {english},
}

@inproceedings{chen_evaluating_2019,
	location = {Hong Kong, China},
	title = {Evaluating Question Answering Evaluation},
	url = {https://www.aclweb.org/anthology/D19-5817},
	doi = {10.18653/v1/D19-5817},
	eventtitle = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
	pages = {119--124},
	booktitle = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Anthony and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	urldate = {2024-01-04},
	date = {2019},
	langid = {english},
	keywords = {todo, useful},
}

@article{ateeq2023arabic,
	title = {Arabic narrative question answering ({QA}) using transformer models},
	journaltitle = {{IEEE} access : practical innovations, open solutions},
	shortjournal = {{IEEE} Access},
	author = {Ateeq, Mohammad A and Tiun, Sabrina and Abdelhaq, Hamed and Rahhal, Nawras},
	date = {2023},
	note = {Publisher: {IEEE}},
}

@article{kovcisky2018narrativeqa,
	title = {The narrativeqa reading comprehension challenge},
	volume = {6},
	pages = {317--328},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Kočiskỳ, Tomáš and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, Gábor and Grefenstette, Edward},
	date = {2018},
	note = {Publisher: {MIT} Press One Rogers Street, Cambridge, {MA} 02142-1209, {USA} journals-info …},
	keywords = {cbt, todo, useful},
}

@article{comrie2008leipzig,
	title = {The Leipzig Glossing Rules: Conventions for interlinear morpheme-by-morpheme glosses},
	volume = {28},
	pages = {2010},
	journaltitle = {Department of Linguistics of the Max Planck Institute for Evolutionary Anthropology \& the Department of Linguistics of the University of Leipzig. Retrieved January},
	author = {Comrie, Bernard and Haspelmath, Martin and Bickel, Balthasar},
	date = {2008},
}

@article{гладуш2019contrastive,
	title = {Contrastive grammar: Theory and practice},
	author = {Гладуш, Надія and Павлюк, Наталія},
	date = {2019},
}

@article{yang_hlt-mt_2022,
	title = {{HLT}-{MT}: High-resource Language-specific Training for Multilingual Neural Machine Translation},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2207.04906},
	doi = {10.48550/ARXIV.2207.04906},
	shorttitle = {{HLT}-{MT}},
	abstract = {Multilingual neural machine translation ({MNMT}) trained in multiple language pairs has attracted considerable attention due to fewer model parameters and lower training costs by sharing knowledge among multiple languages. Nonetheless, multilingual training is plagued by language interference degeneration in shared parameters because of the negative interference among different translation directions, especially on high-resource languages. In this paper, we propose the multilingual translation model with the high-resource language-specific training ({HLT}-{MT}) to alleviate the negative interference, which adopts the two-stage training with the language-specific selection mechanism. Specifically, we first train the multilingual model only with the high-resource pairs and select the language-specific modules at the top of the decoder to enhance the translation quality of high-resource directions. Next, the model is further trained on all available corpora to transfer knowledge from high-resource languages ({HRLs}) to low-resource languages ({LRLs}). Experimental results show that {HLT}-{MT} outperforms various strong baselines on {WMT}-10 and {OPUS}-100 benchmarks. Furthermore, the analytic experiments validate the effectiveness of our method in mitigating the negative interference in multilingual training.},
	author = {Yang, Jian and Yin, Yuwei and Ma, Shuming and Zhang, Dongdong and Li, Zhoujun and Wei, Furu},
	urldate = {2023-12-22},
	date = {2022},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@inproceedings{schonfeld_sitemaps_2009,
	location = {Madrid Spain},
	title = {Sitemaps: above and beyond the crawl of duty},
	isbn = {978-1-60558-487-4},
	url = {https://dl.acm.org/doi/10.1145/1526709.1526842},
	doi = {10.1145/1526709.1526842},
	shorttitle = {Sitemaps},
	eventtitle = {{WWW} '09: The 18th International World Wide Web Conference},
	pages = {991--1000},
	booktitle = {Proceedings of the 18th international conference on World wide web},
	publisher = {{ACM}},
	author = {Schonfeld, Uri and Shivakumar, Narayanan},
	urldate = {2023-12-11},
	date = {2009-04-20},
	langid = {english},
}

@incollection{inbook,
	title = {Analysis of references across wikipedia languages},
	isbn = {978-3-319-67642-5},
	pages = {561--573},
	author = {Lewoniewski, Włodzimierz and Węcel, Krzysztof and Abramowicz, Witold},
	date = {2017-09},
	doi = {10.1007/978-3-319-67642-5_47},
	keywords = {pravda, useful, wikipedia},
}

@article{10.1162/tacl_a_00447,
	title = {Quality at a glance: An audit of web-crawled multilingual datasets},
	volume = {10},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00447},
	doi = {10.1162/tacl_a_00447},
	abstract = {With the success of large-scale pre-training and multilingual modeling in Natural Language Processing ({NLP}), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets ({CCAligned}, {ParaCrawl}, {WikiMatrix}, {OSCAR}, {mC}4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50 \% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.},
	pages = {50--72},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife, Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and Müller, Mathias and Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and de Silva, Nisansa and Çabuk Ballı, Sakine and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
	date = {2022-01},
	note = {tex.eprint: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl{\textbackslash}\_a{\textbackslash}\_00447/1986585/tacl{\textbackslash}\_a{\textbackslash}\_00447.pdf},
	keywords = {crawling, ds, multi-language, useful, web},
}

@misc{11356/1900,
	title = {Ukrainian parliamentary corpus {ParlaMint}-{UA} 4.0.1},
	url = {http://hdl.handle.net/11356/1900},
	author = {Kopp, Matyáš and Kryvenko, Anna and Rii, Andriana},
	date = {2023},
	note = {{ISSN}: 2820-4042
tex.copyright: Creative Commons - Attribution 4.0 International ({CC} {BY} 4.0)},
	keywords = {dataset},
}

@misc{guo_evaluating_2023,
	title = {Evaluating Large Language Models: A Comprehensive Survey},
	url = {http://arxiv.org/abs/2310.19736},
	shorttitle = {Evaluating Large Language Models},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, {LLMs} also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of {LLMs} raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on {LLM} capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of {LLMs}. This survey endeavors to offer a panoramic perspective on the evaluation of {LLMs}. We categorize the evaluation of {LLMs} into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to {LLMs}' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover {LLM} evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of {LLMs}, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of {LLMs}. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-{LLMs}-Evaluation-Papers.},
	number = {{arXiv}:2310.19736},
	publisher = {{arXiv}},
	author = {Guo, Zishan and Jin, Renren and Liu, Chuang and Huang, Yufei and Shi, Dan and Supryadi and Yu, Linhao and Liu, Yan and Li, Jiaxuan and Xiong, Bojian and Xiong, Deyi},
	urldate = {2023-11-09},
	date = {2023-10-31},
	eprinttype = {arxiv},
	eprint = {2310.19736 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, lm, survey, useful},
}

@misc{wiki:xxx,
	title = {List of Wikipedias/Table2 — Meta, discussion about wikimedia projects},
	url = {https://meta.wikimedia.org/w/index.php?title=List_of_Wikipedias/Table2&oldid=23936182},
	author = {{Meta}},
	date = {2022},
}

@misc{enwiki:1182341232,
	title = {Languages used on the internet — Wikipedia, the free encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Languages_used_on_the_Internet&oldid=1182341232},
	author = {{Wikipedia contributors}},
	date = {2023},
}

@article{bender_data_2018,
	title = {Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science},
	volume = {6},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43452},
	doi = {10.1162/tacl_a_00041},
	shorttitle = {Data Statements for Natural Language Processing},
	abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
	pages = {587--604},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {{TACL}},
	author = {Bender, Emily M. and Friedman, Batya},
	urldate = {2023-10-30},
	date = {2018-12},
	langid = {english},
}

@misc{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	number = {{arXiv}:1606.05250},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2023-10-17},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250 [cs]},
	keywords = {Computer Science - Computation and Language, nlp, toread},
}

@misc{rajpurkar_know_2018,
	title = {Know What You Don't Know: Unanswerable Questions for {SQuAD}},
	url = {http://arxiv.org/abs/1806.03822},
	shorttitle = {Know What You Don't Know},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present {SQuAD} 2.0, the latest version of the Stanford Question Answering Dataset ({SQuAD}). {SQuAD} 2.0 combines existing {SQuAD} data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on {SQuAD} 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. {SQuAD} 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on {SQuAD} 1.1 achieves only 66\% F1 on {SQuAD} 2.0.},
	number = {{arXiv}:1806.03822},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	urldate = {2023-10-17},
	date = {2018-06-11},
	eprinttype = {arxiv},
	eprint = {1806.03822 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{synchak2023feminine,
	title = {Feminine personal nouns in ukrainian: Dynamics in a corpus},
	author = {Synchak, Vasyl Starkoand Olena},
	date = {2023},
	keywords = {ua, useful},
}

@inproceedings{9648705,
	title = {Ukrainian text preprocessing in {GRAC}},
	volume = {2},
	doi = {10.1109/CSIT52700.2021.9648705},
	pages = {101--104},
	booktitle = {2021 {IEEE} 16th international conference on computer sciences and information technologies ({CSIT})},
	author = {Starko, Vasyl and Rysin, Andriy and Shvedova, Maria},
	date = {2021},
	keywords = {ocr, ua},
}

@misc{todorov_assessment_2022,
	title = {An Assessment of the Impact of {OCR} Noise on Language Models},
	url = {http://arxiv.org/abs/2202.00470},
	abstract = {Neural language models are the backbone of modern-day natural language processing applications. Their use on textual heritage collections which have undergone Optical Character Recognition ({OCR}) is therefore also increasing. Nevertheless, our understanding of the impact {OCR} noise could have on language models is still limited. We perform an assessment of the impact {OCR} noise has on a variety of language models, using data in Dutch, English, French and German. We find that {OCR} noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as {OCR} quality lowers. In the presence of small corpora, simpler models including {PPMI} and Word2Vec consistently outperform transformer-based models in this respect.},
	number = {{arXiv}:2202.00470},
	publisher = {{arXiv}},
	author = {Todorov, Konstantin and Colavizza, Giovanni},
	urldate = {2023-10-11},
	date = {2022-01-26},
	eprinttype = {arxiv},
	eprint = {2202.00470 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, lm, ocr},
}

@article{bender,
	title = {On achieving and evaluating language-independence in {NLP}},
	volume = {6},
	journaltitle = {Linguistic Issues in Language Technology},
	author = {Bender, Emily M},
	date = {2011},
	keywords = {useful},
}

@collection{wals,
	title = {{WALS} online (v2020.3)},
	url = {https://doi.org/10.5281/zenodo.7385533},
	publisher = {Zenodo},
	editor = {Dryer, Matthew S. and Haspelmath, Martin},
	date = {2013},
	doi = {10.5281/zenodo.7385533},
	note = {Type: Data set},
	keywords = {resource, website},
}

@misc{ruder2020beyondenglish,
	title = {Why you should do {NLP} beyond english},
	url = {http://ruder.io/nlp-beyond-english},
	author = {Ruder, Sebastian},
	date = {2020},
	keywords = {theory, useful},
}

@collection{+2016,
	location = {Warsaw, Poland},
	title = {Language in the digital era. Challenges and perspectives},
	isbn = {978-3-11-047205-9},
	url = {https://doi.org/10.1515/9783110472059},
	publisher = {De Gruyter Open Poland},
	editor = {Dejica, Daniel and Hansen, Gyde and Sandrini, Peter and Para, Iulia},
	urldate = {2023-10-05},
	date = {2016},
	doi = {doi:10.1515/9783110472059},
	keywords = {todo, toread},
}

@misc{bm_lmentry,
	title = {{LMentry}: A language model benchmark of elementary language tasks},
	url = {https://arxiv.org/abs/2211.02069},
	publisher = {{arXiv}},
	author = {Efrat, Avia and Honovich, Or and Levy, Omer},
	date = {2022},
	doi = {10.48550/ARXIV.2211.02069},
	note = {tex.copyright: Creative Commons Attribution 4.0 International},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), benchmark, useful},
}

@misc{ruis_large_2022,
	title = {Large language models are not zero-shot communicators},
	url = {http://arxiv.org/abs/2210.14986},
	abstract = {Despite widespread use of {LLMs} as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response "I wore gloves" to the question "Did you leave fingerprints?" as meaning "No". To investigate whether {LLMs} have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random. Models adapted to be "aligned with human intent" perform much better, but still show a significant gap with human performance. We present our findings as the starting point for further research into evaluating how {LLMs} interpret language in context and to drive the development of more pragmatic and useful models of human discourse.},
	number = {{arXiv}:2210.14986},
	publisher = {{arXiv}},
	author = {Ruis, Laura and Khan, Akbir and Biderman, Stella and Hooker, Sara and Rocktäschel, Tim and Grefenstette, Edward},
	urldate = {2023-10-02},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2210.14986 [cs]},
	keywords = {Computer Science - Computation and Language, lm, theory, toread},
}

@book{__2009,
	title = {Словник мовних покручів},
	publisher = {Товариство ім. Олекси Тихого},
	author = {Тихий, Олексій І and Овсієнко, Василь},
	date = {2009},
	keywords = {resource, ua},
}

@misc{lai_chatgpt_2023,
	title = {{ChatGPT} Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning},
	url = {http://arxiv.org/abs/2304.05613},
	shorttitle = {{ChatGPT} Beyond English},
	abstract = {Over the last few years, large language models ({LLMs}) have emerged as the most important breakthroughs in natural language processing ({NLP}) that fundamentally transform research and developments in the field. {ChatGPT} represents one of the most exciting {LLM} systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for {ChatGPT} in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of {ChatGPT} for English in different problems and areas, a natural question is whether {ChatGPT} can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of {ChatGPT} over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of {ChatGPT} and similar {LLMs} to provide more comprehensive information for multilingual {NLP} applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates {ChatGPT} on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for {ChatGPT} to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of {ChatGPT} for different {NLP} tasks and languages, calling for further research to develop better models and understanding for multilingual learning.},
	number = {{arXiv}:2304.05613},
	publisher = {{arXiv}},
	author = {Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and Nguyen, Thien Huu},
	urldate = {2023-10-02},
	date = {2023-04-12},
	eprinttype = {arxiv},
	eprint = {2304.05613 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ua, useful},
}

@misc{anderson2022essentials,
	title = {Essentials of linguistics, (v. 2.2-February 2023)},
	publisher = {{McMaster} University},
	author = {Anderson, Catherine and Bjorkman, Bronwyn and Denis, Derek and Doner, Julianne and Grant, Margaret and Sanders, Nathan and Taniguchi, Ai},
	date = {2022},
}

@misc{weston_towards_2015,
	title = {Towards {AI}-Complete Question Answering: A Set of Prerequisite Toy Tasks},
	url = {http://arxiv.org/abs/1502.05698},
	shorttitle = {Towards {AI}-Complete Question Answering},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	number = {{arXiv}:1502.05698},
	publisher = {{arXiv}},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	urldate = {2023-09-28},
	date = {2015-12-31},
	eprinttype = {arxiv},
	eprint = {1502.05698 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning, benchmark-task, important},
}

@inproceedings{clark_boolq_2019,
	location = {Minneapolis, Minnesota},
	title = {{BoolQ}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	url = {http://aclweb.org/anthology/N19-1300},
	doi = {10.18653/v1/N19-1300},
	eventtitle = {Proceedings of the 2019 Conference of the North},
	pages = {2924--2936},
	booktitle = {Proceedings of the 2019 Conference of the North},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
	urldate = {2023-09-28},
	date = {2019},
	langid = {english},
}

@misc{liang_xglue_2020,
	title = {{XGLUE}: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},
	url = {http://arxiv.org/abs/2004.01401},
	shorttitle = {{XGLUE}},
	abstract = {In this paper, we introduce {XGLUE}, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to {GLUE}(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, {XGLUE} has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on {XGLUE} as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual {BERT}, {XLM} and {XLM}-R for comparison.},
	number = {{arXiv}:2004.01401},
	publisher = {{arXiv}},
	author = {Liang, Yaobo and Duan, Nan and Gong, Yeyun and Wu, Ning and Guo, Fenfei and Qi, Weizhen and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Fan, Xiaodong and Zhang, Ruofei and Agrawal, Rahul and Cui, Edward and Wei, Sining and Bharti, Taroon and Qiao, Ying and Chen, Jiun-Hung and Wu, Winnie and Liu, Shuguang and Yang, Fan and Campos, Daniel and Majumder, Rangan and Zhou, Ming},
	urldate = {2023-09-28},
	date = {2020-05-22},
	eprinttype = {arxiv},
	eprint = {2004.01401 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{liang_xglue_2020-1,
	title = {{XGLUE}: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},
	url = {http://arxiv.org/abs/2004.01401},
	shorttitle = {{XGLUE}},
	abstract = {In this paper, we introduce {XGLUE}, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to {GLUE}(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, {XGLUE} has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on {XGLUE} as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual {BERT}, {XLM} and {XLM}-R for comparison.},
	number = {{arXiv}:2004.01401},
	publisher = {{arXiv}},
	author = {Liang, Yaobo and Duan, Nan and Gong, Yeyun and Wu, Ning and Guo, Fenfei and Qi, Weizhen and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Fan, Xiaodong and Zhang, Ruofei and Agrawal, Rahul and Cui, Edward and Wei, Sining and Bharti, Taroon and Qiao, Ying and Chen, Jiun-Hung and Wu, Winnie and Liu, Shuguang and Yang, Fan and Campos, Daniel and Majumder, Rangan and Zhou, Ming},
	urldate = {2023-09-28},
	date = {2020-05-22},
	eprinttype = {arxiv},
	eprint = {2004.01401 [cs]},
	keywords = {Computer Science - Computation and Language, benchmark},
}

@article{hamotskyi_benchmark_nodate,
	title = {Benchmark tasks for evaluation of language models},
	rights = {All rights reserved},
	abstract = {Language models ({LMs}) are an integral part of {NLP}, with their importance sharply increasing in recent years with the advent of large generalized {LMs} (such as {OpenAI} {GPT} and {BERT}) that reach and in some cases surpass the level of nonexpert humans. A wide array of methods for {LM} evaluation exist, reﬂecting diﬀerent tasks, model types.},
	author = {Hamotskyi, Serhii},
	langid = {english},
	keywords = {important},
}

@article{ammus,
	title = {{AMMUS} : A survey of transformer-based pretrained models in natural language processing},
	volume = {abs/2108.05542},
	url = {https://arxiv.org/abs/2108.05542},
	journaltitle = {{CoRR}},
	author = {Kalyan, Katikapalli Subramanyam and Rajasekharan, Ajit and Sangeetha, Sivanesan},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2108.05542},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-2108-05542.bib
tex.timestamp: Wed, 18 Aug 2021 19:45:42 +0200},
}

@misc{big,
	title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
	url = {https://arxiv.org/abs/2206.04615},
	publisher = {{arXiv}},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	date = {2022},
	doi = {10.48550/ARXIV.2206.04615},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@inproceedings{benderfr,
	location = {Marseille, France},
	title = {Do we name the languages we study? The \#{BenderRule} in {LREC} and {ACL} articles},
	url = {https://hal.inria.fr/hal-03680561},
	booktitle = {{LREC} 2022 - international conference on language resources and evaluation ({LREC})},
	author = {Ducel, Fanny and Fort, Karën and Lejeune, Gaël and Lepage, Yves},
	date = {2022-06},
	note = {tex.hal\_id: hal-03680561
tex.hal\_version: v1},
	keywords = {\#{BenderRule}, ethics, language diversity},
}

@inproceedings{superglue,
	title = {Superglue: Learning feature matching with graph neural networks},
	pages = {4938--4947},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Sarlin, Paul-Edouard and {DeTone}, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	date = {2020},
}

@inproceedings{gluepaper,
	location = {Brussels, Belgium},
	title = {{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
	url = {https://aclanthology.org/W18-5446},
	doi = {10.18653/v1/W18-5446},
	abstract = {Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most {NLU} models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation ({GLUE}, gluebenchmark.com): a benchmark of nine diverse {NLU} tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. {GLUE} thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in {GLUE} were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use {ELMo} (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.},
	pages = {353--355},
	booktitle = {Proceedings of the 2018 {EMNLP} workshop {BlackboxNLP}: Analyzing and interpreting neural networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	date = {2018-11},
}

@misc{HELM,
	title = {Holistic evaluation of language models},
	url = {https://arxiv.org/abs/2211.09110},
	publisher = {{arXiv}},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	date = {2022},
	doi = {10.48550/ARXIV.2211.09110},
	note = {tex.copyright: Creative Commons Attribution 4.0 International},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), benchmark, important, lm, nlp},
}

@article{gpt2,
	title = {Language models are unsupervised multitask learners},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	date = {2019},
}

@article{lmknowledge,
	title = {Language models as knowledge bases?},
	volume = {abs/1909.01066},
	url = {http://arxiv.org/abs/1909.01066},
	journaltitle = {{CoRR}},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick S. H. and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1909.01066},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-1909-01066.bib
tex.timestamp: Tue, 21 Jan 2020 08:54:02 +0100},
}

@inproceedings{naive,
	location = {Athens, Greece},
	title = {Linguistically naïve != language independent: Why {NLP} needs linguistic typology},
	url = {https://aclanthology.org/W09-0106},
	pages = {26--32},
	booktitle = {Proceedings of the {EACL} 2009 workshop on the interaction between linguistics and computational linguistics: Virtuous, vicious or vacuous?},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M.},
	date = {2009-03},
}

@article{newspaper,
	title = {No news is good news: A critique of the one billion word benchmark},
	journaltitle = {{arXiv} preprint {arXiv}:2110.12609},
	author = {Ngo, Helen and Araújo, João {GM} and Hui, Jeffrey and Frosst, Nicholas},
	date = {2021},
}

@misc{scaling,
	title = {Scaling laws for neural language models},
	url = {https://arxiv.org/abs/2001.08361},
	publisher = {{arXiv}},
	author = {Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	date = {2020},
	doi = {10.48550/ARXIV.2001.08361},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@article{sts,
	title = {Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
	journaltitle = {{arXiv} preprint {arXiv}:1708.00055},
	author = {Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
	date = {2017},
}

@article{benderpost,
	title = {The \#{BenderRule}: On naming the languages we study and why it matters},
	url = {https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/},
	journaltitle = {The Gradient},
	author = {Bender, Emily},
	date = {2019},
}

@misc{lambada,
	title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
	url = {https://arxiv.org/abs/1606.06031},
	publisher = {{arXiv}},
	author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
	date = {2016},
	doi = {10.48550/ARXIV.1606.06031},
	note = {tex.copyright: {arXiv}.org perpetual, non-exclusive license},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@article{inclusion,
	title = {The state and fate of linguistic diversity and inclusion in the {NLP} world},
	volume = {abs/2004.09095},
	url = {https://arxiv.org/abs/2004.09095},
	journaltitle = {{CoRR}},
	author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2004.09095},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-2004-09095.bib
tex.timestamp: Wed, 22 Apr 2020 12:57:53 +0200},
}

@inproceedings{winograd,
	title = {The winograd schema challenge},
	booktitle = {Thirteenth international conference on the principles of knowledge representation and reasoning},
	author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
	date = {2012},
}

@inproceedings{loster2017improving,
	title = {Improving company recognition from unstructured text by using dictionaries.},
	pages = {610--619},
	booktitle = {{EDBT}},
	author = {Loster, Michael and Zuo, Zhe and Naumann, Felix and Maspfuhl, Oliver and Thomas, Dirk},
	date = {2017},
}
